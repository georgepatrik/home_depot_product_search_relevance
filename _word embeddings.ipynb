{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of _word embeddings.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOdikeodOoi2d1SifCc6pyX"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"cOVSFfxNVu84","colab_type":"code","outputId":"5fa9eed4-78ba-4c70-c345-b301ee0669a4","executionInfo":{"status":"ok","timestamp":1588433454324,"user_tz":-180,"elapsed":2183,"user":{"displayName":"George Patrickios","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinVNxP4BoMpJRz4UXFAm1qbufjykFhjlJWi-_vFw=s64","userId":"09709844844784386152"}},"colab":{"base_uri":"https://localhost:8080/","height":833}},"source":["import gensim\n","import pandas as pd\n","import numpy as np\n","import nltk \n","nltk.download(\"popular\")\n","import re\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, GradientBoostingRegressor, GradientBoostingClassifier\n","from sklearn.model_selection import cross_validate\n","from sklearn.metrics import mean_squared_error, make_scorer\n","from math import sqrt\n","from nltk.stem.snowball import SnowballStemmer\n","import time\n","import sklearn.feature_extraction.text as sktf\n","from scipy.spatial.distance import cosine"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading collection 'popular'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Package cmudict is already up-to-date!\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Package gazetteers is already up-to-date!\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Package genesis is already up-to-date!\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Package gutenberg is already up-to-date!\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Package inaugural is already up-to-date!\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package movie_reviews is already up-to-date!\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Package names is already up-to-date!\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Package shakespeare is already up-to-date!\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Package treebank is already up-to-date!\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package twitter_samples is already up-to-date!\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Package omw is already up-to-date!\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Package wordnet is already up-to-date!\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Package wordnet_ic is already up-to-date!\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Package words is already up-to-date!\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package snowball_data is already up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection popular\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"clMUsHsxnuK4","colab_type":"code","colab":{}},"source":["#function for stemming and number of common words \n","\n","#regular expresions, stop words removal and stemmer\n","stemmer = SnowballStemmer('english')\n","stop_words = set(stopwords.words('english')) \n","\n","def str_cleaner_stemmer(s):\n","    s_clean = (re.sub(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)',\" \", s))\n","    return \" \".join([stemmer.stem(word) for word in s_clean.lower().split() if word not in stop_words])\n"," \n","#function regarding the pre-defined dictionary of spelling mistakes for improved efficiency.\n","def spell_check(text):\n","    for key in spell_check_dict:\n","        text = text.replace(key, spell_check_dict[key])\n","    return text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4QDaeHb8VVnd","colab_type":"code","colab":{}},"source":["df_train_data = pd.read_csv('/content/drive/My Drive/IHU/NLP/Coursework/data/train.csv', encoding=\"ISO-8859-1\")\n","df_test_data = pd.read_csv('/content/drive/My Drive/IHU/NLP/Coursework/data/test.csv', encoding=\"ISO-8859-1\")\n","df_attr = pd.read_csv('/content/drive/My Drive/IHU/NLP/Coursework/data/attributes.csv',encoding='ISO-8859-1')\n","df_pro_desc_data = pd.read_csv('/content/drive/My Drive/IHU/NLP/Coursework/data/product_descriptions.csv',encoding='ISO-8859-1')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5YDEzKQXPMvM","colab_type":"code","colab":{}},"source":["#Create a bullets dataframe which combines the bullets from attributes.csv\n","\n","#list with names of the attributes Bullet01 to Bullet22\n","#Bullet22 is the max bullet attribute found in this dataset\n","bullet_list = []\n","for i in range (1,23):\n","  if i<10:\n","    i = '0'+str(i)\n","  bullet_list.append('Bullet'+str(i))\n","\n","#Keep only the rows with the bullet attribute for each product_uid\n","df_bullets_rows = df_attr[df_attr.name.isin(bullet_list)][[\"product_uid\",'name', \"value\"]]\n","\n","#transpose the dataframe so Bullet rows become Columns \n","#every row now is the product with its values for every bullet (if any)\n","#fillna with the empty string since not all prodcuts have values in all the bullets\n","df_bullets_t = (df_bullets_rows.pivot(index='product_uid', columns='name', values='value')\n","                                .fillna('').reset_index())\n","\n","#create a column which aggreagates all bullets to one column \n","df_bullets_t['bullets'] = df_bullets_t[bullet_list].agg(' '.join, axis=1)\n","\n","#keep only the combined \n","df_bullets = df_bullets_t.filter(['product_uid','bullets'], axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bBmIyd5nu3o_","colab_type":"code","colab":{}},"source":["#Load ready-dictionary which checks spelling for search terms\n","spell_check_dict= {}\n","\n","with open(\"/content/drive/My Drive/IHU/NLP/Coursework/data/spelling_dictionary.txt\") as f:\n","  for line in f:\n","    spell_check_dict[str(line.split(\":\")[0])] = str(line.split(\":\")[1]).strip(\"\\n\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"77sDntowWMqv","colab_type":"code","colab":{}},"source":["#Concat the train and test data in on DF for efficiency \n","#Be carefull sort must be false in order to maintain the correct order\n","df_all = pd.concat((df_train_data, df_test_data), axis=0, ignore_index=True, sort = False)\n","\n","#Merge with product description on product uid \n","df_all = pd.merge(df_all, df_pro_desc_data, how='left', on='product_uid')\n","\n","#Merge with material on product uid\n","df_all = pd.merge(df_all, df_bullets, how='left', on='product_uid')\n","df_all['bullets'] = df_all['bullets'].replace(np.nan,\" \")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dUnyiItPu-Wx","colab_type":"code","colab":{}},"source":["#Spell Check\n","df_all['search_term'] = df_all['search_term'].map(lambda x:spell_check(str(x)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iAbaOygtvmvc","colab_type":"code","colab":{}},"source":["#Perform Cleaning and Stemming at string fields\n","df_all['search_term'] = df_all['search_term'].map(lambda x:str_cleaner_stemmer(x))\n","df_all['product_title'] = df_all['product_title'].map(lambda x:str_cleaner_stemmer(x))\n","df_all['product_description'] = df_all['product_description'].map(lambda x:str_cleaner_stemmer(x))\n","df_all['bullets'] = df_all['bullets'].map(lambda x:str_cleaner_stemmer(x))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0BYyYv4Mm9Rv","colab_type":"code","colab":{}},"source":["#Create Product Info column which will be use for creating the documents\n","#for the vocalbulary genration \n","df_all['product_info'] = df_all['product_description']+\" \"+df_all['search_term']+\" \"+df_all['product_title']+\" \"+df_all['bullets']\n","\n","\n","#Create the docunments each document consists of the search term + prod title\n","documents =[]\n","\n","for i in df_all['product_info']:\n","  documents.append(list(i.split()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2fCVXG8kP0RG","colab_type":"code","colab":{}},"source":["# build vocabulary and train model\n","model = gensim.models.Word2Vec(\n","        documents,\n","        size=150,\n","        window=25,\n","        min_count=0,\n","        workers=10,\n","        iter=5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"byJ37H0OP0J2","colab_type":"code","outputId":"7e6f1015-7c7d-49f8-dff8-78d6111aebfa","executionInfo":{"status":"ok","timestamp":1588436829707,"user_tz":-180,"elapsed":1061530,"user":{"displayName":"George Patrickios","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinVNxP4BoMpJRz4UXFAm1qbufjykFhjlJWi-_vFw=s64","userId":"09709844844784386152"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["model.train(documents,total_examples=len(documents),epochs=5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(178847859, 188828655)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"VHsSjdsoTdZ0","colab_type":"code","outputId":"23a1117a-a62a-4fa2-beaa-1580e1680353","executionInfo":{"status":"ok","timestamp":1588437028510,"user_tz":-180,"elapsed":1260266,"user":{"displayName":"George Patrickios","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinVNxP4BoMpJRz4UXFAm1qbufjykFhjlJWi-_vFw=s64","userId":"09709844844784386152"}},"colab":{"base_uri":"https://localhost:8080/","height":241}},"source":["#Create the mean vectors for the search term and prodcuct title\n","# using word2vec from the model created above\n","\n","df_all['search_term_mean_vector'] = df_all['search_term'].map(lambda x: np.mean([model[w] for w in x.split()], axis = 0))\n","df_all['prod_title_mean_vector'] = df_all['product_title'].map(lambda x: np.mean([model[w] for w in x.split()], axis = 0))\n","df_all['prod_desc_mean_vector'] = df_all['product_description'].map(lambda x: np.mean([model[w] for w in x.split()], axis = 0))\n","df_all['bullets_mean_vector'] = df_all['bullets'].map(lambda x: np.mean([model[w] for w in x.split()], axis = 0))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  \n","/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n","  out=out, **kwargs)\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  \"\"\"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"oDlZwSBJw8ZG","colab_type":"code","colab":{}},"source":["#Compute the cosine similarity between the mean vectors\n","df_all['st_pt_vector_similarity'] = df_all.apply(lambda row: 1 - cosine(row['search_term_mean_vector'], row['prod_title_mean_vector']), axis=1)\n","df_all['st_pd_vector_similarity'] = df_all.apply(lambda row: 1 - cosine(row['search_term_mean_vector'], row['prod_desc_mean_vector']), axis=1)\n","df_all['st_bt_vector_similarity'] = df_all.apply(lambda row: 1 - cosine(row['search_term_mean_vector'], row['bullets_mean_vector']), axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lwjN9LtC7j2U","colab_type":"code","colab":{}},"source":["#Export to csv\n","from google.colab import files\n","\n","export_columns = ['id','st_pt_vector_similarity', 'st_pd_vector_similarity','st_bt_vector_similarity']\n","df_all.loc[:,export_columns].to_csv('vector_similarity.csv', index = False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AU6jv_g0eaB5","colab_type":"code","colab":{}},"source":["# files.download(\"vector_similarity.csv\")\n","\n","## Save csv directly to drive\n","# !vector_similarity.csv \"drive/My Drive/IHU/NLP/Coursework/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o556A09AkZYD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}